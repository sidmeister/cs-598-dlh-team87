{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ef8aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import glove\n",
    "from glove import Corpus\n",
    "\n",
    "import collections\n",
    "import gc \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9224902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_notes = pd.read_pickle(\"data/ner_df.p\") # med7\n",
    "w2vec = Word2Vec.load(\"embeddings/word2vec.model\")\n",
    "fasttext = FastText.load(\"embeddings/fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431c14af",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_index_list = []\n",
    "for i in new_notes.itertuples():\n",
    "    \n",
    "    if len(i.ner) == 0:\n",
    "        null_index_list.append(i.Index)\n",
    "new_notes.drop(null_index_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12600579",
   "metadata": {},
   "outputs": [],
   "source": [
    "med7_ner_data = {}\n",
    "\n",
    "for ii in new_notes.itertuples():\n",
    "    \n",
    "    p_id = ii.SUBJECT_ID\n",
    "    ind = ii.Index\n",
    "    \n",
    "    try:\n",
    "        new_ner = new_notes.loc[ind].ner\n",
    "    except:\n",
    "        new_ner = []\n",
    "            \n",
    "    unique = set()\n",
    "    new_temp = []\n",
    "    \n",
    "    for j in new_ner:\n",
    "        for k in j:\n",
    "            \n",
    "            unique.add(k[0])\n",
    "            new_temp.append(k)\n",
    "\n",
    "    if p_id in med7_ner_data:\n",
    "        for i in new_temp:\n",
    "            med7_ner_data[p_id].append(i)\n",
    "    else:\n",
    "        med7_ner_data[p_id] = new_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7a014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(med7_ner_data, \"data/new_ner_word_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ec5ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(a):\n",
    "    return sum(a) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa699027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2vec starting..\n",
      "fasttext starting..\n",
      "combined starting..\n",
      "22025 22025 22446\n"
     ]
    }
   ],
   "source": [
    "data_types = [med7_ner_data]\n",
    "data_names = [\"new_ner\"]\n",
    "\n",
    "for data, names in zip(data_types, data_names):\n",
    "    new_word2vec = {}\n",
    "    print(\"w2vec starting..\")\n",
    "    for k,v in data.items():\n",
    "\n",
    "        patient_temp = []\n",
    "        for i in v:\n",
    "            try:\n",
    "                patient_temp.append(w2vec.wv[i[0]])\n",
    "            except:\n",
    "                avg = []\n",
    "                num = 0\n",
    "                temp = []\n",
    "\n",
    "                if len(i[0].split(\" \")) > 1:\n",
    "                    for each_word in i[0].split(\" \"):\n",
    "                        try:\n",
    "                            temp = w2vec[each_word]\n",
    "                            avg.append(temp)\n",
    "                            num += 1\n",
    "                        except:\n",
    "                            pass\n",
    "                    if num == 0: continue\n",
    "                    avg = np.asarray(avg)\n",
    "                    t = np.asarray(map(mean, zip(*avg)))\n",
    "                    patient_temp.append(t)\n",
    "        if len(patient_temp) == 0: continue\n",
    "        new_word2vec[k] = patient_temp\n",
    "\n",
    "    #############################################################################\n",
    "    print(\"fasttext starting..\")\n",
    "        \n",
    "    new_fasttextvec = {}\n",
    "\n",
    "    for k,v in data.items():\n",
    "\n",
    "        patient_temp = []\n",
    "\n",
    "        for i in v:\n",
    "            try:\n",
    "                patient_temp.append(fasttext.wv[i[0]])\n",
    "            except:\n",
    "                pass\n",
    "        if len(patient_temp) == 0: continue\n",
    "        new_fasttextvec[k] = patient_temp\n",
    "\n",
    "    #############################################################################    \n",
    "        \n",
    "    print(\"combined starting..\")\n",
    "    new_concatvec = {}\n",
    "\n",
    "    for k,v in data.items():\n",
    "        patient_temp = []\n",
    "        if k == '6':\n",
    "            print('key found')\n",
    "            continue\n",
    "        for i in v:\n",
    "            w2vec_temp = []\n",
    "            try:\n",
    "                w2vec_temp = w2vec.wv[i[0]]\n",
    "            except:\n",
    "                avg = []\n",
    "                num = 0\n",
    "                temp = []\n",
    "\n",
    "                if len(i[0].split(\" \")) > 1:\n",
    "                    for each_word in i[0].split(\" \"):\n",
    "                        try:\n",
    "                            temp = w2vec.wv[each_word]\n",
    "                            avg.append(temp)\n",
    "                            num += 1\n",
    "                        except:\n",
    "                            pass\n",
    "                    if num == 0: \n",
    "                        w2vec_temp = [0] * 100\n",
    "                    elif num ==1:\n",
    "                        w2vec_temp = temp\n",
    "                    else:\n",
    "                        avg = np.asarray(avg)\n",
    "#                         w2vec_temp = np.asarray(map(mean, zip(*avg)))\n",
    "                        w2vec_temp = avg.mean(axis=0)\n",
    "                    \n",
    "                else:\n",
    "                    w2vec_temp = [0] * 100\n",
    "\n",
    "#             fasttemp = fasttext[i[0]]\n",
    "\n",
    "            try:\n",
    "               fasttemp = fasttext.wv[i[0]]\n",
    "            except:\n",
    "               fasttemp = [0] * 100            \n",
    "            appended = np.append(fasttemp, w2vec_temp, 0)\n",
    "            patient_temp.append(appended)\n",
    "        if len(patient_temp) == 0: continue\n",
    "        new_concatvec[k] = patient_temp\n",
    "\n",
    "    print(len(new_word2vec), len(new_fasttextvec), len(new_concatvec))\n",
    "    pd.to_pickle(new_word2vec, \"data/\"+names+\"_word2vec_dict.pkl\")\n",
    "    pd.to_pickle(new_fasttextvec, \"data/\"+names+\"_fasttext_dict.pkl\")\n",
    "    pd.to_pickle(new_concatvec, \"data/\"+names+\"_combined_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01595a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fasttext_dict = pd.read_pickle(\"data/new_ner_fasttext_dict.pkl\") \n",
    "new_word2vec_dict = pd.read_pickle(\"data/new_ner_word2vec_dict.pkl\")\n",
    "new_combined_dict = pd.read_pickle(\"data/new_ner_combined_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc5ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths before: 22025, 22025, 22025\n",
      "Lengths after:  22025, 22025, 22025\n"
     ]
    }
   ],
   "source": [
    "new_fasttextvec_keys  = set(new_fasttextvec.keys())\n",
    "new_word2vec_keys     = set(new_word2vec.keys())\n",
    "new_concatvec_keys    = set(new_concatvec.keys())\n",
    "intersection_keys     = new_fasttextvec_keys.intersection(new_word2vec_keys).intersection(new_concatvec_keys)\n",
    "print(f\"Lengths before: {len(new_word2vec)}, {len(new_fasttextvec)}, { len(new_concatvec)}\")\n",
    "for i in new_fasttextvec_keys - intersection_keys:\n",
    "    del new_fasttextvec[i]\n",
    "for i in new_word2vec_keys - intersection_keys:\n",
    "    del new_word2vec[i]\n",
    "for i in new_concatvec_keys - intersection_keys:\n",
    "    del new_concatvec[i]\n",
    "print(f\"Lengths after:  {len(new_word2vec)}, {len(new_fasttextvec)}, {len(new_concatvec)}\")\n",
    "\n",
    "pd.to_pickle(new_word2vec, \"data/\"+\"new_ner\"+\"_word2vec_limited_dict.pkl\")\n",
    "pd.to_pickle(new_fasttextvec, \"data/\"+\"new_ner\"+\"_fasttext_limited_dict.pkl\")\n",
    "pd.to_pickle(new_concatvec, \"data/\"+\"new_ner\"+\"_combined_limited_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afc54f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
