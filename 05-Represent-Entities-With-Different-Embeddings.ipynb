{"cells":[{"cell_type":"code","execution_count":1,"id":"53ef8aee","metadata":{},"outputs":[],"source":"import pandas as pd\nimport os\nimport numpy as np\nfrom gensim.models import Word2Vec, FastText\nimport glove\nfrom glove import Corpus\n\nimport random\n\nimport collections\nimport gc \n\nimport warnings\nwarnings.filterwarnings('ignore')"},{"cell_type":"code","execution_count":2,"id":"9224902b","metadata":{},"outputs":[],"source":"new_notes = pd.read_pickle(\"data/ner_df.p\") # med7\nw2vec = Word2Vec.load(\"embeddings/word2vec.model\")\nfasttext = FastText.load(\"embeddings/fasttext.model\")"},{"cell_type":"code","execution_count":3,"id":"431c14af","metadata":{},"outputs":[],"source":"null_index_list = []\nfor i in new_notes.itertuples():\n    \n    if len(i.ner) == 0:\n        null_index_list.append(i.Index)\nnew_notes.drop(null_index_list, inplace=True)"},{"cell_type":"code","execution_count":4,"id":"12600579","metadata":{},"outputs":[],"source":"med7_ner_data = {}\n\nfor ii in new_notes.itertuples():\n    \n    p_id = ii.SUBJECT_ID\n    ind = ii.Index\n    \n    try:\n        new_ner = new_notes.loc[ind].ner\n    except:\n        new_ner = []\n            \n    unique = set()\n    new_temp = []\n    \n    for j in new_ner:\n        for k in j:\n            \n            unique.add(k[0])\n            new_temp.append(k)\n\n    if p_id in med7_ner_data:\n        for i in new_temp:\n            med7_ner_data[p_id].append(i)\n    else:\n        med7_ner_data[p_id] = new_temp"},{"cell_type":"code","execution_count":5,"id":"2b7a014c","metadata":{},"outputs":[],"source":"pd.to_pickle(med7_ner_data, \"data/new_ner_word_dict.pkl\")"},{"cell_type":"code","execution_count":6,"id":"7ec5ef44","metadata":{},"outputs":[],"source":"def mean(a):\n    return sum(a) / len(a)"},{"cell_type":"code","execution_count":7,"id":"aa699027","metadata":{},"outputs":[],"source":"data_types = [med7_ner_data]\ndata_names = [\"new_ner\"]\n\nfor data, names in zip(data_types, data_names):\n    new_word2vec = {}\n    print(\"w2vec starting..\")\n    for k,v in data.items():\n\n        patient_temp = []\n        for i in v:\n            try:\n                patient_temp.append(w2vec.wv[i[0]])\n            except:\n                avg = []\n                num = 0\n                temp = []\n\n                if len(i[0].split(\" \")) > 1:\n                    for each_word in i[0].split(\" \"):\n                        try:\n                            temp = w2vec[each_word]\n                            avg.append(temp)\n                            num += 1\n                        except:\n                            pass\n                    if num == 0: continue\n                    avg = np.asarray(avg)\n                    t = np.asarray(map(mean, zip(*avg)))\n                    patient_temp.append(t)\n        if len(patient_temp) == 0: continue\n        new_word2vec[k] = patient_temp\n\n    #############################################################################\n    print(\"fasttext starting..\")\n        \n    new_fasttextvec = {}\n\n    for k,v in data.items():\n\n        patient_temp = []\n\n        for i in v:\n            try:\n                patient_temp.append(fasttext.wv[i[0]])\n            except:\n                pass\n        if len(patient_temp) == 0: continue\n        new_fasttextvec[k] = patient_temp\n\n    #############################################################################    \n        \n    print(\"combined starting..\")\n    new_concatvec = {}\n\n    for k,v in data.items():\n        patient_temp = []\n        if k == '6':\n            print('key found')\n            continue\n        for i in v:\n            w2vec_temp = []\n            try:\n                w2vec_temp = w2vec.wv[i[0]]\n            except:\n                avg = []\n                num = 0\n                temp = []\n\n                if len(i[0].split(\" \")) > 1:\n                    for each_word in i[0].split(\" \"):\n                        try:\n                            temp = w2vec.wv[each_word]\n                            avg.append(temp)\n                            num += 1\n                        except:\n                            pass\n                    if num == 0: \n                        w2vec_temp = [0] * 100\n                    elif num ==1:\n                        w2vec_temp = temp\n                    else:\n                        avg = np.asarray(avg)\n#                         w2vec_temp = np.asarray(map(mean, zip(*avg)))\n                        w2vec_temp = avg.mean(axis=0)\n                    \n                else:\n                    w2vec_temp = [0] * 100\n\n#             fasttemp = fasttext[i[0]]\n\n            try:\n               fasttemp = fasttext.wv[i[0]]\n            except:\n               fasttemp = [0] * 100            \n            appended = np.append(fasttemp, w2vec_temp, 0)\n            patient_temp.append(appended)\n        if len(patient_temp) == 0: continue\n        new_concatvec[k] = patient_temp\n\n    print(len(new_word2vec), len(new_fasttextvec), len(new_concatvec))\n    pd.to_pickle(new_word2vec, \"data/\"+names+\"_word2vec_dict.pkl\")\n    pd.to_pickle(new_fasttextvec, \"data/\"+names+\"_fasttext_dict.pkl\")\n    pd.to_pickle(new_concatvec, \"data/\"+names+\"_combined_dict.pkl\")"},{"cell_type":"code","execution_count":8,"id":"01595a02","metadata":{},"outputs":[],"source":"new_fasttext_dict = pd.read_pickle(\"data/new_ner_fasttext_dict.pkl\") \nnew_word2vec_dict = pd.read_pickle(\"data/new_ner_word2vec_dict.pkl\")\nnew_combined_dict = pd.read_pickle(\"data/new_ner_combined_dict.pkl\")"},{"cell_type":"code","execution_count":9,"id":"2acc5ae2","metadata":{},"outputs":[],"source":"new_fasttextvec_keys  = set(new_fasttextvec.keys())\nnew_word2vec_keys     = set(new_word2vec.keys())\nnew_concatvec_keys    = set(new_concatvec.keys())\nintersection_keys     = new_fasttextvec_keys.intersection(new_word2vec_keys).intersection(new_concatvec_keys)\n\nintersection_keys = set(random.sample(intersection_keys, 10000)) \n\nprint(f\"Lengths before: {len(new_word2vec)}, {len(new_fasttextvec)}, { len(new_concatvec)}\")\nfor i in new_fasttextvec_keys - intersection_keys:\n    del new_fasttextvec[i]\nfor i in new_word2vec_keys - intersection_keys:\n    del new_word2vec[i]\nfor i in new_concatvec_keys - intersection_keys:\n    del new_concatvec[i]\nprint(f\"Lengths after:  {len(new_word2vec)}, {len(new_fasttextvec)}, {len(new_concatvec)}\")\n\npd.to_pickle(new_word2vec, \"data/\"+\"new_ner\"+\"_word2vec_limited_dict.pkl\")\npd.to_pickle(new_fasttextvec, \"data/\"+\"new_ner\"+\"_fasttext_limited_dict.pkl\")\npd.to_pickle(new_concatvec, \"data/\"+\"new_ner\"+\"_combined_limited_dict.pkl\")"},{"cell_type":"code","execution_count":null,"id":"1afc54f8","metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}