{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-04-23 20:35:20.580782: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-04-23 20:35:20.645867: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-04-23 20:35:20.646760: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-04-23 20:35:21.569826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":"import pandas as pd\nimport os\nimport numpy as np\nfrom gensim.models import Word2Vec, FastText\nimport glove\nfrom glove import Corpus\nimport time\n\nimport collections\nimport gc \n\nimport keras\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.models import Sequential, Model\nfrom keras.layers import Flatten, Dense, Dropout, Input, concatenate, Concatenate, Activation, Concatenate, LSTM, GRU\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\nfrom keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D, Concatenate\n\nfrom tensorflow.keras.optimizers import Adam\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\nfrom keras.utils import np_utils\n# from keras.backend.tensorflow_backend import set_session, clear_session, get_session\nfrom keras.backend import set_session, clear_session, get_session\nimport tensorflow as tf\n\n\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\ndef mean(a):\n    return sum(a) / len(a)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"# Reset Keras Session\ndef reset_keras(model):\n    sess = get_session()\n    clear_session()\n    sess.close()\n    sess = get_session()\n\n    try:\n        del model # this is from global space - change this as you need\n    except:\n        pass\n\n    gc.collect() # if it's done something you should see a number being outputted\n\ndef make_prediction_timeseries(model, test_data):\n    probs = model.predict(test_data)\n    y_pred = [1 if i>=0.5 else 0 for i in probs]\n    return probs, y_pred\n\ndef save_scores_timeseries(predictions, probs, ground_truth, model_name, \n                problem_type, iteration, hidden_unit_size, type_of_ner):\n    \n    auc = roc_auc_score(ground_truth, probs)\n    auprc = average_precision_score(ground_truth, probs)\n    acc   = accuracy_score(ground_truth, predictions)\n    F1    = f1_score(ground_truth, predictions)\n    \n    \n    result_dict = {}    \n    result_dict['auc'] = auc\n    result_dict['auprc'] = auprc\n    result_dict['acc'] = acc\n    result_dict['F1'] = F1\n\n        \n    file_name = str(hidden_unit_size)+\"-\"+model_name+\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\".p\"\n    \n    result_path = \"results/timeseries-baseline/\"\n    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n\n    print(auc, auprc, acc, F1)"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"def timeseries_model(layer_name, number_of_unit):\n    K.clear_session()\n    \n    sequence_input = Input(shape=(24,104),  name = \"timeseries_input\")\n    \n    if layer_name == \"LSTM\":\n        x = LSTM(number_of_unit)(sequence_input)\n    else:\n        x = GRU(number_of_unit)(sequence_input)\n    \n    logits_regularizer = tf.keras.regularizers.L2(0.01)\n    sigmoid_pred = Dense(1, activation='sigmoid',use_bias=False,\n                         kernel_initializer=tf.keras.initializers.glorot_normal(), \n                  kernel_regularizer=logits_regularizer)(x)\n    \n    \n    model = Model(inputs=sequence_input, outputs=sigmoid_pred)\n    \n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n    return model"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":"type_of_ner = \"new\"\n\nx_train_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_train.pkl\")\nx_dev_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_dev.pkl\")\nx_test_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_test.pkl\")\n\ny_train = pd.read_pickle(\"data/\"+type_of_ner+\"_y_train.pkl\")\ny_dev = pd.read_pickle(\"data/\"+type_of_ner+\"_y_dev.pkl\")\ny_test = pd.read_pickle(\"data/\"+type_of_ner+\"_y_test.pkl\")"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"SyntaxError","evalue":"EOL while scanning string literal (<ipython-input-1-26239e90c478>, line 56)","output_type":"error","traceback":["\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-26239e90c478>\"\u001b[1;36m, line \u001b[1;32m56\u001b[0m\n\u001b[1;33m    with open(\") as timer_file:\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"]}],"source":"timer_file_path = \"/home/ubuntu/notebook/time_results/07/\"\n\nmodel_patience = 3\nmonitor_criteria = 'val_loss'\nbatch_size = 128\n\nepoch_num = 100\nunit_sizes = [128, 256]\niter_num = 11\ntarget_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\nlayers = [\"LSTM\", \"GRU\"]\n\n\n# epoch_num = 2\n# iter_num = 2\n# target_problems = ['mort_hosp']\n# layers = [\"GRU\"]\n# unit_sizes = [256]\n\ntime_saver = dict()\n\nfor each_layer in layers:\n    print(\"Layer: \", each_layer)\n    for each_unit_size in unit_sizes:\n        print(\"Hidden unit: \", each_unit_size)\n        for cycle in range(0, iter_num):\n            iteration = cycle + 1\n            print(\"Iteration number: \", iteration)\n            print(\"=============================\")\n\n            for each_problem in target_problems:\n                print (\"Problem type: \", each_problem)\n                print (\"__________________\")\n\n                initial_time = time.time()\n\n                # early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n                # best_model_name = str(each_layer)+\"-\"+str(each_unit_size)+\"-\"+str(each_problem)+\"-\"+\"best_model.hdf5\"\n                # checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1,\n                #     save_best_only=True, mode='min', save_freq='epoch')\n\n                # callbacks = [early_stopping_monitor, checkpoint]\n\n                # model = timeseries_model(each_layer, each_unit_size)\n                # model.fit(x_train_lstm, y_train[each_problem], epochs=epoch_num, verbose=1, \n                #           validation_data=(x_dev_lstm, y_dev[each_problem]), callbacks=callbacks, batch_size= batch_size)\n\n                # model.load_weights(best_model_name)\n\n                # probs, predictions = make_prediction_timeseries(model, x_test_lstm)\n                # save_scores_timeseries(predictions, probs, y_test[each_problem].values,str(each_layer),\n                #                        each_problem, iteration, each_unit_size,type_of_ner)\n                # reset_keras(model)\n                # #del model\n                # clear_session()\n                # gc.collect()\n\n                final_time = time.time() - initial_time\n                with open(f\"{timer_file_path}{each_layer}_{each_unit_size}_{iteration}_{each_problem}\") as timer_file: \n                    if each_layer not in time_saver: \n                        time_saver[each_layer] = dict()\n                    if each_unit_size not in time_saver[each_layer]:\n                        time_saver[each_layer][each_unit_size] = dict()\n                    if each_problem not in time_saver[each_layer][each_unit_size]: \n                        time_saver[each_layer][each_unit_size][each_problem] = list()\n                    \n                    time_saver[each_layer][each_unit_size][each_problem].append(final_time)\n                    timer_file.write(final_time)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"for layer in time_saver:\n    for unit in time_saver[layer]: \n        for problem in time_saver[layer][unit]:\n            print(time_saver[layer][unit][problem])"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}