{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import pandas as pd\nimport os\nimport numpy as np\nfrom gensim.models import Word2Vec, FastText\nimport glove\nfrom glove import Corpus\nimport collections\nimport gc \nimport keras\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.models import Sequential, Model\nfrom keras.layers import Flatten, Dense, Dropout, Input, concatenate, Concatenate, Activation, Concatenate, LSTM, GRU\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\nfrom keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D, Concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\nfrom keras.utils import np_utils\n# from keras.backend.tensorflow_backend import set_session, clear_session, get_session\nfrom keras.backend import set_session, clear_session, get_session\nimport tensorflow as tf\nfrom keras.optimizers import Adam\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\nimport warnings\nwarnings.filterwarnings('ignore')\nimport time\ndef mean(a):\n    return sum(a) / len(a)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Reset Keras Session\ndef reset_keras(model):\n    sess = get_session()\n    clear_session()\n    sess.close()\n    sess = get_session()\n\n    try:\n        del model # this is from global space - change this as you need\n    except:\n        pass\n\n    gc.collect() # if it's done something you should see a number being outputted\n\ndef create_dataset(dict_of_ner):\n    temp_data = []\n    for k, v in sorted(dict_of_ner.items()):\n        temp = []\n        for embed in v:\n            temp.append(embed)\n        temp_data.append(np.mean(temp, axis = 0)) \n    return np.asarray(temp_data)\n\ndef make_prediction_multi_avg(model, test_data):\n    probs = model.predict(test_data)\n    y_pred = [1 if i>=0.5 else 0 for i in probs]\n    return probs, y_pred\n\ndef save_scores_multi_avg(predictions, probs, ground_truth, \n                          \n                          embed_name, problem_type, iteration, hidden_unit_size,\n                          \n                          sequence_name, type_of_ner):\n    \n    auc = roc_auc_score(ground_truth, probs)\n    auprc = average_precision_score(ground_truth, probs)\n    acc   = accuracy_score(ground_truth, predictions)\n    F1    = f1_score(ground_truth, predictions)\n    \n    result_dict = {}    \n    result_dict['auc'] = auc\n    result_dict['auprc'] = auprc\n    result_dict['acc'] = acc\n    result_dict['F1'] = F1\n    \n    result_path = \"results/multimodal-baseline\"\n    file_name = str(sequence_name)+\"-\"+str(hidden_unit_size)+\"-\"+embed_name\n    file_name = file_name +\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\"-avg-.p\"\n    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n\n    print(auc, auprc, acc, F1)\n    \ndef avg_ner_model(layer_name, number_of_unit, embedding_name):\n\n    if embedding_name == \"concat\":\n        input_dimension = 200\n    elif embedding_name == \"biobert\": \n        input_dimension = 768\n    else:\n        input_dimension = 100\n\n    sequence_input = Input(shape=(24,104))\n\n    input_avg = Input(shape=(input_dimension, ), name = \"avg\")        \n#     x_1 = Dense(256, activation='relu')(input_avg)\n#     x_1 = Dropout(0.3)(x_1)\n    \n    if layer_name == \"GRU\":\n        x = GRU(number_of_unit)(sequence_input)\n    elif layer_name == \"LSTM\":\n        x = LSTM(number_of_unit)(sequence_input)\n\n    x = keras.layers.Concatenate()([x, input_avg])\n\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    \n    \n#     logits_regularizer = tf.contrib.layers.l2_regularizer(scale=0.01)\n    \n#     preds = Dense(1, activation='sigmoid',use_bias=False,\n#                          kernel_initializer=tf.contrib.layers.xavier_initializer(), \n#                   kernel_regularizer=logits_regularizer)(x)\n    \n    \n# https://stackoverflow.com/questions/70447036/run-the-code-tensorflow-1-15-in-tensorflow-2-7\n    logits_regularizer = tf.keras.regularizers.l2(l=0.5 * (0.01))\n    preds = Dense(1, activation='sigmoid',use_bias=False,kernel_initializer=tf.compat.v1.keras.initializers.glorot_normal(), \n                  kernel_regularizer=logits_regularizer)(x)\n    \n    adam_opt = Adam(lr=0.0006, decay=0.01)\n        \n    model = Model(inputs=[sequence_input, input_avg], outputs=preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=adam_opt,\n                  metrics=['acc'])\n    \n    return model"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"type_of_ner = \"new\"\n\nx_train_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_train.pkl\")\nx_dev_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_dev.pkl\")\nx_test_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_test.pkl\")\n\ny_train = pd.read_pickle(\"data/\"+type_of_ner+\"_y_train.pkl\")\ny_dev = pd.read_pickle(\"data/\"+type_of_ner+\"_y_dev.pkl\")\ny_test = pd.read_pickle(\"data/\"+type_of_ner+\"_y_test.pkl\")\n\nner_word2vec = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_word2vec_limited_dict.pkl\")\nner_fasttext = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_fasttext_limited_dict.pkl\")\nner_concat = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_combined_limited_dict.pkl\")\n# ner_biobert = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_biobert_limited_dict.pkl\")\n\nner_biobert = pd.read_pickle(\"data/biobert_test_data.pkl\")\n\ntrain_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_train_ids.pkl\")\ndev_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_dev_ids.pkl\")\ntest_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_test_ids.pkl\")"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"\nmonitor_criteria = 'val_loss'\nmodel_patience = 5\nbatch_size = 64\n\n\n# num_epoch = 100\ntarget_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\niter_num = 2\n# layers = [\"LSTM\", \"GRU\"]\n# unit_sizes = [128, 256]\nembedding_types = ['word2vec', 'fasttext', 'concat','biobert']\nembedding_dict = [ner_word2vec, ner_fasttext, ner_concat, ner_biobert]\n\n#embedding_types = ['word2vec', 'fasttext', 'concat']\n#embedding_dict = [ner_word2vec, ner_fasttext, ner_concat]\n\n\nnum_epoch = 100\nlayers = [\"GRU\"]\nunit_sizes = [256]\n\n\ntotal_model_time_initial = time.time()\nembedding_timer = {}\n\nfor item in embedding_types:\n    embedding_timer[item] = list()\n\nindex = 0\n\nfor each_layer in layers:\n    print (\"Layer: \", each_layer)\n    for each_unit_size in unit_sizes:\n        print (\"Hidden unit: \", each_unit_size)\n\n        for embed_dict, embed_name in zip(embedding_dict, embedding_types):\n            print (\"Embedding: \", embed_name)\n            print(\"=============================\")\n            \n            \n\n            temp_train_ner = dict((k, embed_dict[k]) for k in train_ids)\n            temp_dev_ner = dict((k, embed_dict[k]) for k in dev_ids)\n            temp_test_ner = dict((k, embed_dict[k]) for k in test_ids)\n\n            x_train_ner = create_dataset(temp_train_ner)\n            x_dev_ner = create_dataset(temp_dev_ner)\n            x_test_ner = create_dataset(temp_test_ner)\n\n\n            for iteration in range(1, iter_num):\n                print (\"Iteration number: \", iteration)\n\n                for each_problem in target_problems:\n                    print (\"Problem type: \", each_problem)\n                    print (\"__________________\")\n                    \n                    initial_timer = time.time()\n\n                    early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n                    best_model_name = \"avg-\"+str(embed_name)+\"-\"+str(each_problem)+\"-\"+\"best_model.hdf5\"\n                    checkpoint = ModelCheckpoint(best_model_name, monitor='val_loss', verbose=1,\n                        save_best_only=True, mode='min', save_freq='epoch')\n\n\n                    callbacks = [early_stopping_monitor, checkpoint]\n\n                    model = avg_ner_model(each_layer, each_unit_size, embed_name)\n\n                    model.fit([x_train_lstm, x_train_ner], y_train[each_problem], epochs=num_epoch, verbose=1, \n                              validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), callbacks=callbacks, \n                              batch_size=batch_size )\n                \n                    model.load_weights(best_model_name)\n\n                    probs, predictions = make_prediction_multi_avg(model, [x_test_lstm, x_test_ner])\n                    \n                    save_scores_multi_avg(predictions, probs, y_test[each_problem], \n                               embed_name, each_problem, iteration, each_unit_size, \n                               each_layer, type_of_ner)\n                    \n                    reset_keras(model)\n                    #del model\n                    clear_session()\n                    gc.collect()\n\n                    embedding_timer[embed_name].append(time.time()-initial_timer)\n                    with open(f\"/home/ubuntu/times_{embed_name}_{each_problem}.txt\", \"w\") as file1:\n                        file1.write(f\"time to run {embed_name}-{each_problem}: {embedding_timer[embed_name][len(embedding_timer[embed_name]) -1]}\")\nprint(f\"final model training time: {time.time() - total_model_time_initial}\")"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# a"},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 2, 3, 4]\n"]}],"source":"embedding_types = ['word2vec', 'fasttext', 'concat','biobert']\n\nfor item in embedding_types:\n    print(f\"{embedding_timer[item]}\")\n    \nprint(f\"final model training time: {time.time() - total_model_time_initial}\")"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}