{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import pandas as pd\nimport os\nimport numpy as np\nfrom gensim.models import Word2Vec, FastText\nimport glove\nfrom glove import Corpus\n\nimport collections\nimport gc \n\nimport time\n\nimport keras\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.models import Sequential, Model\nfrom keras.layers import Flatten, Dense, Dropout, Input, concatenate, Concatenate, Activation, Concatenate, LSTM, GRU\nfrom keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv1D, BatchNormalization, GRU, Convolution1D, LSTM\nfrom keras.layers import UpSampling1D, MaxPooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D,MaxPool1D, Concatenate\nfrom tensorflow.keras.optimizers import Adam\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, History, ReduceLROnPlateau\nfrom keras.utils import np_utils\nfrom tensorflow.compat.v1.keras.backend import set_session, clear_session, get_session\nimport tensorflow as tf\n\n\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\ndef mean(a):\n    return sum(a) / len(a)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"type_of_ner = \"new\"\n\nx_train_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_train.pkl\")\nx_dev_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_dev.pkl\")\nx_test_lstm = pd.read_pickle(\"data/\"+type_of_ner+\"_x_test.pkl\")\n\ny_train = pd.read_pickle(\"data/\"+type_of_ner+\"_y_train.pkl\")\ny_dev = pd.read_pickle(\"data/\"+type_of_ner+\"_y_dev.pkl\")\ny_test = pd.read_pickle(\"data/\"+type_of_ner+\"_y_test.pkl\")\n\n\nner_word2vec = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_word2vec_limited_dict.pkl\")\nner_fasttext = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_fasttext_limited_dict.pkl\")\nner_concat = pd.read_pickle(\"data/\"+type_of_ner+\"_ner_combined_limited_dict.pkl\")\n\ntrain_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_train_ids.pkl\")\ndev_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_dev_ids.pkl\")\ntest_ids = pd.read_pickle(\"data/\"+type_of_ner+\"_test_ids.pkl\")"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def make_prediction_cnn(model, test_data):\n    probs = model.predict(test_data)\n    y_pred = [1 if i>=0.5 else 0 for i in probs]\n    return probs, y_pred\n\ndef save_scores_cnn(predictions, probs, ground_truth, \n                          \n                          embed_name, problem_type, iteration, hidden_unit_size,\n                          \n                          sequence_name, type_of_ner):\n    \n    auc = roc_auc_score(ground_truth, probs)\n    auprc = average_precision_score(ground_truth, probs)\n    acc   = accuracy_score(ground_truth, predictions)\n    F1    = f1_score(ground_truth, predictions)\n    \n    result_dict = {}    \n    result_dict['auc'] = auc\n    result_dict['auprc'] = auprc\n    result_dict['acc'] = acc\n    result_dict['F1'] = F1\n\n    result_path = \"results/cnn/\"\n    file_name = str(sequence_name)+\"-\"+str(hidden_unit_size)+\"-\"+embed_name\n    file_name = file_name +\"-\"+problem_type+\"-\"+str(iteration)+\"-\"+type_of_ner+\"-cnn-.p\"\n    pd.to_pickle(result_dict, os.path.join(result_path, file_name))\n\n    print(auc, auprc, acc, F1)\n    \ndef print_scores_cnn(predictions, probs, ground_truth, model_name, problem_type, iteration, hidden_unit_size):\n    auc = roc_auc_score(ground_truth, probs)\n    auprc = average_precision_score(ground_truth, probs)\n    acc   = accuracy_score(ground_truth, predictions)\n    F1    = f1_score(ground_truth, predictions)\n    \n    print (\"AUC: \", auc, \"AUPRC: \", auprc, \"F1: \", F1)\n    \ndef get_subvector_data(size, embed_name, data):\n    if embed_name == \"concat\":\n        vector_size = 200\n    else:\n        vector_size = 100\n\n    x_data = {}\n    for k, v in data.items():\n        number_of_additional_vector = len(v) - size\n        vector = []\n        for i in v:\n            vector.append(i)\n        if number_of_additional_vector < 0: \n            number_of_additional_vector = np.abs(number_of_additional_vector)\n\n            temp = vector[:size]\n            for i in range(0, number_of_additional_vector):\n                temp.append(np.zeros(vector_size))\n            x_data[k] = np.asarray(temp)\n        else:\n            x_data[k] = np.asarray(vector[:size])\n\n    return x_data\n\n\ndef proposedmodel(layer_name, number_of_unit, embedding_name, ner_limit, num_filter):\n    if embedding_name == \"concat\":\n        input_dimension = 200\n    else:\n        input_dimension = 100\n\n    sequence_input = Input(shape=(24,104))\n\n    input_img = Input(shape=(ner_limit, input_dimension), name = \"cnn_input\")\n\n    convs = []\n    filter_sizes = [2,3,4]\n\n\n\n    text_conv1d = Conv1D(filters=num_filter, kernel_size=3, \n                 padding = 'valid', strides = 1, dilation_rate=1, activation='relu', \n                         kernel_initializer=tf.keras.initializers.glorot_normal() )(input_img)\n    \n    text_conv1d = Conv1D(filters=num_filter*2, kernel_size=3, \n                 padding = 'valid', strides = 1, dilation_rate=1, activation='relu',\n                        kernel_initializer=tf.keras.initializers.glorot_normal())(text_conv1d)   \n    \n    text_conv1d = Conv1D(filters=num_filter*3, kernel_size=3, \n                 padding = 'valid', strides = 1, dilation_rate=1, activation='relu',\n                        kernel_initializer=tf.keras.initializers.glorot_normal())(text_conv1d)   \n\n    \n    #concat_conv = keras.layers.Concatenate()([text_conv1d, text_conv1d_2, text_conv1d_3])\n    text_embeddings = GlobalMaxPooling1D()(text_conv1d)\n    #text_embeddings = Dense(128, activation=\"relu\")(text_embeddings)\n    \n    if layer_name == \"GRU\":\n        x = GRU(number_of_unit)(sequence_input)\n    elif layer_name == \"LSTM\":\n        x = LSTM(number_of_unit)(sequence_input)\n\n    #concatenated = keras.layers.Concatenate()([x, text_embeddings])\n    concatenated = keras.layers.concatenate([x, text_embeddings], axis=1)\n\n    concatenated = Dense(512, activation='relu')(concatenated)\n    concatenated = Dropout(0.2)(concatenated)\n    #concatenated = Dense(256, activation='relu')(concatenated)\n    #concatenated = Dense(512, activation='relu')(concatenated)\n    \n    #concatenated = Dense(512, activation='relu')(concatenated)\n    logits_regularizer = tf.keras.regularizers.L2(0.01)\n    preds = Dense(1, activation='sigmoid',use_bias=False,\n                         kernel_initializer=tf.keras.initializers.glorot_normal(), \n                  kernel_regularizer=logits_regularizer)(concatenated)\n    \n    \n    #opt = Adam(lr=1e-4, decay = 0.01)\n    \n#     opt = Adam(lr=1e-3, decay = 0.01)\n    \n    opt = Adam(lr=0.001)\n\n    model = Model(inputs=[sequence_input, input_img], outputs=preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=opt,\n                  metrics=['acc'])\n    \n    return model\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"timer_file_path = \"/home/ubuntu/notebooks/time_results/09/\"\ntime_saver = dict()\n\n\nembedding_types = ['word2vec', 'fasttext', 'concat']\nembedding_dict = [ner_word2vec, ner_fasttext, ner_concat]\nbatch_size = 64\nmonitor_criteria = 'val_loss'\n\n\n\n# target_problems = ['mort_hosp', 'mort_icu', 'los_3', 'los_7']\n# num_epoch = 100\n# model_patience = 5\n# maxiter = 11\n\nfilter_number = 32\nner_representation_limit = 64\nactivation_func = \"relu\"\n\n\nsequence_model = \"GRU\"\nsequence_hidden_unit = 256\n\n\n\ntarget_problems = ['mort_hosp']\nnum_epoch = 2\nmodel_patience = 5\niterations = 1\nfor embed_dict, embed_name in zip(embedding_dict, embedding_types):    \n    print (\"Embedding: \", embed_name)\n    print(\"=============================\")\n    \n    temp_train_ner = dict((k, embed_dict[k]) for k in train_ids)\n    tem_dev_ner = dict((k, embed_dict[k]) for k in dev_ids)\n    temp_test_ner = dict((k, embed_dict[k]) for k in test_ids)\n\n    x_train_dict = {}\n    x_dev_dict = {}\n    x_test_dict = {}\n\n    x_train_dict = get_subvector_data(ner_representation_limit, embed_name, temp_train_ner)\n    x_dev_dict = get_subvector_data(ner_representation_limit, embed_name, tem_dev_ner)\n    x_test_dict = get_subvector_data(ner_representation_limit, embed_name, temp_test_ner)\n\n    x_train_dict_sorted = collections.OrderedDict(sorted(x_train_dict.items()))\n    x_dev_dict_sorted = collections.OrderedDict(sorted(x_dev_dict.items()))\n    x_test_dict_sorted = collections.OrderedDict(sorted(x_test_dict.items()))\n    \n    x_train_ner = np.asarray(list(x_train_dict_sorted.values()))\n    x_dev_ner = np.asarray(list(x_dev_dict_sorted.values()))\n    x_test_ner = np.asarray(list(x_test_dict_sorted.values())) \n        \n    for cycle in range(0, iterations):\n        iteration = cycle + 1\n        print (\"Iteration number: \", iteration)\n    \n        for each_problem in target_problems:\n            print (\"Problem type: \", each_problem)\n            print (\"__________________\")\n            \n            initial_time = time.time()\n            \n            early_stopping_monitor = EarlyStopping(monitor=monitor_criteria, patience=model_patience)\n            \n            best_model_name = str(ner_representation_limit)+\"-basiccnn1d-\"+str(embed_name)+\"-\"+str(each_problem)+\"-\"+\"best_model.hdf5\"\n            \n            checkpoint = ModelCheckpoint(best_model_name, monitor=monitor_criteria, verbose=1,\n                save_best_only=True, mode='min')\n            \n            reduce_lr = ReduceLROnPlateau(monitor=monitor_criteria, factor=0.2,\n                              patience=2, min_lr=0.00001, epsilon=1e-4, mode='min')\n            \n\n            callbacks = [early_stopping_monitor, checkpoint, reduce_lr]\n            \n            #model = textCNN(sequence_model, sequence_hidden_unit, embed_name, ner_representation_limit)\n            model = proposedmodel(sequence_model, sequence_hidden_unit, \n                               embed_name, ner_representation_limit,filter_number)\n            model.fit([x_train_lstm, x_train_ner], y_train[each_problem], epochs=num_epoch, verbose=1, \n                      validation_data=([x_dev_lstm, x_dev_ner], y_dev[each_problem]), callbacks=callbacks, batch_size=batch_size)\n            \n            \n            probs, predictions = make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n            print_scores_cnn(predictions, probs, y_test[each_problem], embed_name, each_problem, iteration, sequence_hidden_unit)\n            \n            model.load_weights(best_model_name)\n                      \n            probs, predictions = make_prediction_cnn(model, [x_test_lstm, x_test_ner])\n            save_scores_cnn(predictions, probs, y_test[each_problem], embed_name, each_problem, iteration,\n                            sequence_hidden_unit, sequence_model, type_of_ner)\n            del model\n            clear_session()\n            gc.collect()\n            \n            final_time = time.time() - initial_time\n            \n            if embed_name not in time_saver:\n                time_saver[embed_name] = dict()\n                \n            if each_problem not in time_saver[embed_name]: \n                time_saver[embed_name][each_problem] = list()\n            \n            time_saver[embed_name][each_problem].append(str(final_time))\n            \n            with open(f\"{timer_file_path}{embed_name}_{each_problem}_{iteration}\", \"w\") as file:\n                file.write(str(final_time))"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"for embed_name in time_saver:\n    for problem in  time_saver[embed_name]:\n        print(f\"{embed_name}-{each_problem}: {time_saver[embed_name][each_problem]}\")"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}